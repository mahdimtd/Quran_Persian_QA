# app.py
import streamlit as st
import os

# ----------- ÙÙ‚Ø· Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø±Ø§ unzip Ú©Ù† -----------
if not os.path.exists("my_chroma_index"):
    import zipfile
    with zipfile.ZipFile("my_chroma_index.zip", "r") as zip_ref:
        zip_ref.extractall("my_chroma_index")

# ----------- Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ Ùˆ Ù„ÙˆØ¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ -----------
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext, load_index_from_storage
import chromadb
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI

PERSIST_DIRECTORY = "my_chroma_index"
COLLECTION_NAME = "my_collection"
EMBED_MODEL_NAME = "PartAI/Tooka-SBERT-V2-Large"

# ----------- Ù„ÙˆØ¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ù…Ø¯Ù„ Embedding -----------
@st.cache_resource(show_spinner=False)
def load_index_and_embed():
    chroma_client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    chroma_collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = load_index_from_storage(storage_context)
    embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)
    return index, embed_model

index, embed_model = load_index_and_embed()

# ----------- Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ OpenRouter Ùˆ System Prompt -----------
OPENROUTER_API_KEY = "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"  # Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†
OPENROUTER_MODEL = "deepseek/deepseek-chat-v3-0324:free"

SYSTEM_PROMPT = """
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø¯Ù‡.
Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ Ø¯Ù‚ÛŒÙ‚ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯: Â«Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.Â»
Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ø§Ø´Ø¯ Ùˆ ÙÚ©Ø± Ú©Ø±Ø¯Ù†Øª Ø±Ø§ Ù‡Ù… Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù†Ù…Ø§ÛŒØ´ Ù†Ø¯Ù‡

"""

llm = OpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    model=OPENROUTER_MODEL,
    is_chat_model=True,
    system_prompt=SYSTEM_PROMPT,
)

# ----------- UI Streamlit -----------
st.set_page_config(page_title="Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ØªÙ†ÛŒ", page_icon="ğŸ“š")
st.title("ğŸ“š Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ØªÙ†ÛŒ")

question = st.text_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")

if question:
    with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´..."):
        query_engine = index.as_query_engine(embed_model=embed_model, llm=llm)
        response = query_engine.query(question)
        st.markdown("#### Ù¾Ø§Ø³Ø®:")
        st.write(str(response))
